{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chap08_Cm01",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNws8JtRsTsj6GaioxRdfwc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MasatsuguA/EU_M_Math_spring/blob/main/chap08_Cm01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "CxjvCuGtiXs9",
        "outputId": "589ec049-2ce0-480f-f8d8-5f08e7397b9b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-b65c0b9cac9e>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    “回帰“とは、結果に影響を与える情報を使って、知りたい値を導き出す機械学習の分析を行うこと。定義した変数に、欠損値が無いか確認し、とある物から説明変数からtipを取り除き、目的変数をtipだけに絞っていく。目的変数を予測するために、説明変数に加えて係数と切片を利用していく\u001b[0m\n\u001b[0m                                                                                                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
          ]
        }
      ],
      "source": [
        "#chap08_Cm01\n",
        "回帰\n",
        "“回帰“とは、結果に影響を与える情報を使って、知りたい値を導き出す機械学習の分析を行うこと。定義した変数に、欠損値が無いか確認し、とある物から説明変数からtipを取り除き、目的変数をtipだけに絞っていく。目的変数を予測するために、説明変数に加えて係数と切片を利用していく\n",
        "参考文献\n",
        "https://smart-hint.com/ml/linear-regression/\n",
        "\n",
        "分類\n",
        "機械学習においては、離散的な入力値を、事前に定義された複数のクラスに分類することを指す。離散的な入力値とは、例えば画像からの犬猫判定を考えると、何らかの方法で数値化してグラフ上に点としてプロットすると、線状ではなく、ばらばらの点々になってしまうと考えられるが、このようなデータのこと。この場合は、“犬の点の集まり”や“猫の点の集まり”の中間に境界線を引いて区分けすることで、犬猫判定の分類が実現可能となる。\n",
        "参考文献\n",
        "https://atmarkit.itmedia.co.jp/ait/articles/1901/06/news030.html\n",
        "\n",
        "教師有り学習\n",
        "機械学習手法の一つで有り、AI人工知能に関わる技術の一つ。複数のデータより規則性を見つけて、それに基づいて予想や判断を行う。\n",
        "コンピューターにデータを与えて、そのデータの規則性を学習させる。\n",
        "画像認識の面では飛行機の画像に対して、飛行機であるという事をコンピューターに与える。与えられた画像の特徴(主翼やエンジンなど)とその合否の関連性を学習することによって、“これはXX”と言う正解が分っていない“○○の画像”というデータを与えられてもそれが何かを判断できる物になる。\n",
        "参考文献\n",
        "https://www.headboost.jp/python-machine-learning-first-step/\n",
        "\n",
        "重回帰分析\n",
        "単回帰分析の入力変数を1つから複数に増やしたもの。それにより、単回帰から、以下のような変化がある。“行列を使った計算が増える”。“複数の入力変数の粒度を揃えるために正規化が必要”、“単回帰と同様の計算に対して、入力変数の数に応じた補正が必要になる場合がある”\n",
        "参考文献\n",
        "https://qiita.com/karaage0703/items/f38d18afc1569fcc0418\n",
        "\n",
        "ロジスティック回帰\n",
        "ある現象の発生する確率Nを予想する統計学的手法である。説明変数を入力した際に、ロジスティック関数を元に発生確率Nを求めて、最終的に0か1を繰り返すモデル。\n",
        "例えば“飛行機の部品”について”適合＝1“、”不適合＝0“と2パターンで占めせる。\n",
        "参考文献\n",
        "https://di-acc2.com/analytics/ai/16440/\n",
        "\n",
        "正則化\n",
        "モデルの複雑さの罰則を差すために導入され滑らかでないことに罰則をかけたり、パラメータのノルム(平面や空間における幾何学ベクトルの長さの概念の一般かであり、ベクトル空間に対しての距離を与える数学の道具。定義されたベクトル空間を線型ノルム空間)の大きさに罰則をかけたりする物である。\n",
        "参考文献\n",
        "https://tech-clips.com/over-fitting-and-regularization-in-python#i-2\n",
        "\n",
        "リッジ回帰\n",
        "過学習を防ぐため線形回帰に正則化項としてL2ノルムを導入したモデル。過学習を抑える手法の一つとも言えます。 真の関数を cos 関数としノイズを乗せてプロットした点に多項式でフィッティングする。新しい入力に対して正確な予想ができません。ここで過学習を起こしているモデルのパラメータを確認すると、パラメータ値が大きくなっていることがわかる。過学習の特徴として学習したパラメータ値が大きくなる性質がある\n",
        "参考文献\n",
        "htps://qiita.com/K_Noguchi/items/3f5cf527d6f6d46767fb\n",
        "\n",
        "\n",
        "ラッソ回帰\n",
        "重回帰分析を過学習が起こりにくいように改良したもので、重回帰分析では、損失関数が最小になるように回帰係数を推定するが加えて、回帰係数そのものが大きくなることを避ける工夫が施されている。損失関数に変数の数や重みが増えるほどペナルティが加算されるようにして、モデル自身にパラメータの大きさを抑制してもらいる。\n",
        "参考文献\n",
        "https://qiita.com/y_itoh/items/9befbf47869d66337dad\n",
        "\n",
        "決定木\n",
        "条件による分岐を“根”からたどることで、最も条件に合致する“葉”を検索するアルゴリズムである。学習データをもとに説明変数から成る条件式をノードとして作成すると、”葉”の部分で予測結果を導き出せるようなモデルが自動作成される。“分類問題”、“回帰問題”のどちらにも対応が可能である。メリットとデメリットは下記のようなものがありますが、解釈が容易というメリットが最も決定木を使用する理由になるメリット、デメリットとがある。\n",
        "\n",
        "参考文献\n",
        "https://qiita.com/g-k/items/9c1307274814d651752a\n",
        "\n",
        "エントロピー\n",
        "エントロピーは“乱雑さ”を表し、熱力学や統計力学でよく使用される。また、エントロピーは画像の情報量を表す指標としても使用されている。画像のエントロピーの定義は以下のようになる。\n",
        "参考文献\n",
        "https://qiita.com/toshi_machine/items/05db9a2af21b8ffc6f21\n",
        "\n",
        "情報利得\n",
        "親ノードから子ノードへグループを分けたときにそこから得られる情報量\n",
        "\n",
        "参考文献\n",
        "https://darden.hatenablog.com/entry/2016/12/09/221630\n",
        "\n",
        "k-NN\n",
        "クラス未知のサンプルのクラスラベルを近傍サンプル k 個による多数決で決めようというシンプルな手法\n",
        "参考文献\n",
        "https://qiita.com/oirom/items/22ccb7c0139dce925f43\n",
        "\n",
        "SVM\n",
        "教師あり学習を用いるパターン認識モデルの一つであり、分類や回帰へ適用できる。サポートベクターマシンは、現在知られている手法の中でも認識性能が優れた学習モデルの一つである。サポートベクターマシンが優れた認識性能を発揮することができる理由は、未学習データに対して高い識別性能を得るための工夫がある。\n",
        "参考文献\n",
        "https://qiita.com/ryu1104/items/06a71d8f6956da889bb8\n",
        "\n",
        "\n",
        "ノーフリーランチ\n",
        "コスト関数の局地を探索するアルゴリズムは全ての可能なコスト関数に適用した結果を平均すると同じ性能である。特定の問題に限定されず、どのような問題に対して対応できるように設計された。\n",
        "参考文献\n",
        "https://www.hellocybernetics.tech/entry/2016/05/20/014838#"
      ]
    }
  ]
}